{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project is in fulfilment of an imagined  requirement to create a data pipeline to populate a data warehouse for the data scientist team at Widgets Inc. The data warehouse is dedicated to the purpose of collecting various data about state regions within the United States for the purpose of data analysis by the data science team.\n",
    "\n",
    "This project has also been deployed to Github, and is available at XXXXX\n",
    "\n",
    "The data pipeline will follow these essential steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The data warehouse will be populated by Pyspark as an engine for various ETL jobs (Extract, Tranform, and Load). At some points, Pandas will be used for data frame manipulation. The Pyspark pipeline will produce source-of-truth tables and one analytics summary table in order to facilitate the jobs of the staff data scientist team.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The data will be sourced from the following locations:\n",
    "- Listing of states and state abbreviations for states in the United States of America, plus the District of Columbia. \n",
    "- I94 Immigration Data\n",
    "- U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import configparser\n",
    "import datetime as dt\n",
    "import time\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear, avg, monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData, HiveContext\n",
    "#from pyspark.sql.functions import col,isnan, when, count\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.2 Import: State names and abbreviations\n",
    "- Sourced from World Population Review (https://worldpopulationreview.com/states/state-abbreviations)\n",
    "- Provided in JSON format\n",
    "- 51 total records (50 states + District of Columbia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|state_code|          state_name|\n",
      "+----------+--------------------+\n",
      "|        AK|              Alaska|\n",
      "|        AL|             Alabama|\n",
      "|        AR|            Arkansas|\n",
      "|        AZ|             Arizona|\n",
      "|        CA|          California|\n",
      "|        CO|            Colorado|\n",
      "|        CT|         Connecticut|\n",
      "|        DC|District Of Columbia|\n",
      "|        DE|            Delaware|\n",
      "|        FL|             Florida|\n",
      "|        GA|             Georgia|\n",
      "|        HI|              Hawaii|\n",
      "|        IA|                Iowa|\n",
      "|        ID|               Idaho|\n",
      "|        IL|            Illinois|\n",
      "|        IN|             Indiana|\n",
      "|        KS|              Kansas|\n",
      "|        KY|            Kentucky|\n",
      "|        LA|           Louisiana|\n",
      "|        MA|       Massachusetts|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_url = \"https://worldpopulationreview.com/static/states/abbr-name.json\"\n",
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(json_url)\n",
    "\n",
    "states_json_df = spark.read.option(\"multiline\",\"true\").option(\"inferSchema\",\"true\").json(\"file://\"+SparkFiles.get(\"abbr-name.json\"))\n",
    "states_pandas = states_json_df.toPandas()\n",
    "\n",
    "statesArr = []\n",
    "for val in states_pandas:\n",
    "    statesArr.append({'state_code': str(val), 'state_name': states_pandas[val][0]})\n",
    "\n",
    "states_pandas = pd.DataFrame(statesArr, columns=['state_code', 'state_name'])\n",
    "states_df=spark.createDataFrame(states_pandas) \n",
    "states_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.3 Import: I94 Immigration data.\n",
    "- Provided by the the US National Tourism and Trade Office\n",
    "- 3,096,313 total records\n",
    "- CSV source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "file_name = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_df = spark.read.format('com.github.saurfang.sas.spark').load(file_name)\n",
    "immigration_df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.4 Import: US city demographic data\n",
    "- Provided by OpenSoft, via a US Census Bureau's 2015 American Community Survey.\n",
    "- 2,891 total records\n",
    "- CSV source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8            40601   \n",
       "1            Quincy  Massachusetts        41.0            44129   \n",
       "2            Hoover        Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga     California        34.5            88127   \n",
       "4            Newark     New Jersey        34.6           138040   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0              41862             82463                1562         30908   \n",
       "1              49500             93629                4147         32935   \n",
       "2              46799             84839                4819          8229   \n",
       "3              87105            175232                5821         33878   \n",
       "4             143873            281913                5829         86253   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load U.S. City Demographic Data\n",
    "file_name = \"us-cities-demographics.csv\"\n",
    "demographics_df = spark.read.csv(file_name, inferSchema=True, header=True, sep=';')\n",
    "\n",
    "# display the first five records\n",
    "demographics_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Below, we will explore the data to identify data quality issues, like missing values and duplicate data, and document steps necessary to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Review schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Temperature schema\n",
    "states_df.printSchema()\n",
    "immigration_df.printSchema()\n",
    "demographics_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Look for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_nulls(input_df):\n",
    "    '''\n",
    "    Looks for missing data in input dataframe, and returns a dataframe with the number of missing values in each column'\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        input dataFrame\n",
    "    '''\n",
    "\n",
    "    input_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in input_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following null values in States data\n",
      "+----------+----------+\n",
      "|state_code|state_name|\n",
      "+----------+----------+\n",
      "|         0|         0|\n",
      "+----------+----------+\n",
      "\n",
      "Found the following null values in Immigration data\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|    0|       1| 1881250|3088187|    238| 138429|3095921| 138429|    802|    477|414269|2982605|  83627|     0|19549|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n",
      "Found the following null values in Demographics data\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|   0|    0|         0|              3|                3|               0|                13|          13|                    16|         0|   0|    0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Found the following null values in States data\")\n",
    "get_nulls(states_df)\n",
    "print(\"Found the following null values in Immigration data\")\n",
    "get_nulls(immigration_df)\n",
    "print(\"Found the following null values in Demographics data\")\n",
    "get_nulls(demographics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Look for duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 duplicates from states data\n",
      "Found 0 duplicates from immigration data\n",
      "Found 0 duplicates from demographics data\n"
     ]
    }
   ],
   "source": [
    "print(\"Found \" + str(states_df.groupBy(\"state_code\").count().filter(\"count > 1\").count()) + \" duplicates from states data\")\n",
    "print(\"Found \" + str(immigration_df.groupBy(\"cicid\").count().filter(\"count > 1\").count()) + \" duplicates from immigration data\")\n",
    "print(\"Found \" + str(demographics_df.groupBy(\"City\", \"State\", \"Race\").count().filter(\"count > 1\").count()) +  \" duplicates from demographics data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Plan cleaning steps\n",
    "\n",
    "We have shown that there are no duplicate values from the source data. Therefore, the only steps required for cleaning the data is to remove the null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.1 Conceptual Data Model\n",
    "The data will be modelled as a Star Schema for better support for OLAP queries, i.e. for overall design elegance and optimized use in data analytics. The fact table and dimension tables are documented below.\n",
    "\n",
    "##### Fact Table\n",
    "\n",
    "*state_analysis*\n",
    "- state_code\n",
    "- avg_percent_foreign_born\n",
    "- avg_percent_veterans\n",
    "- avg_household_size\n",
    "- avg_median_age\n",
    "- recent_period_total_immigrants\n",
    "- recent_period_total_immigrants_month\n",
    "- recent_period_total_immigrants_year\n",
    "\n",
    "\n",
    "##### Dimension Tables\n",
    "\n",
    "*states*\n",
    "- state_code\n",
    "- title\n",
    "\n",
    "*city_demographics*\n",
    "- city\n",
    "- state_code\n",
    "- total_population\n",
    "- total_foreign_born\n",
    "- percent_foreign_born\n",
    "- total_veterans\n",
    "- percent_veterans\n",
    "- household_size\n",
    "- median_age\n",
    "\n",
    "*i94_immigration_monthly*\n",
    "- i94_id\n",
    "- year\n",
    "- month\n",
    "- city_port_code\n",
    "- state_code\n",
    "- birth_year\n",
    "- gender\n",
    "- visa_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "- Create dimension tables as subsets of the relevant data from source files.\n",
    "- Perform data clean up, as following the requirements documented in Step 2.\n",
    "- Create fact table, generated as summary analytics from dimension tables\n",
    "- Save final parquet files from each table created.\n",
    "- Run quality control checks on the final parquet files.\n",
    "- Summarize output as data dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run ETL to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1 Create dimension tables as subsets of the relevant data from source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark SQL views\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# Create Dimension tables\n",
    "states_df.createOrReplaceTempView(\"states\")\n",
    "immigration_df.createOrReplaceTempView(\"i94_immigration_monthly_source\")\n",
    "demographics_df.createOrReplaceTempView(\"city_demographics_source\")\n",
    "\n",
    "#allow unlimited time for SQL joins and parquet writes.\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreshold\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+--------------+----------+----------+------+---------+\n",
      "|i94_id|  year|month|city_port_code|state_code|birth_year|gender|visa_type|\n",
      "+------+------+-----+--------------+----------+----------+------+---------+\n",
      "|   6.0|2016.0|  4.0|           XXX|      null|    1979.0|  null|       B2|\n",
      "|   7.0|2016.0|  4.0|           ATL|        AL|    1991.0|     M|       F1|\n",
      "|  15.0|2016.0|  4.0|           WAS|        MI|    1961.0|     M|       B2|\n",
      "|  16.0|2016.0|  4.0|           NYC|        MA|    1988.0|  null|       B2|\n",
      "|  17.0|2016.0|  4.0|           NYC|        MA|    2012.0|  null|       B2|\n",
      "+------+------+-----+--------------+----------+----------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        cicid as i94_id, i94yr as year, i94mon as month, i94port as city_port_code,\n",
    "        i94addr as state_code, biryear as birth_year, gender, visatype as visa_type\n",
    "    FROM i94_immigration_monthly_source\n",
    "\"\"\").toDF('i94_id', 'year', 'month', 'city_port_code', 'state_code', 'birth_year', 'gender', 'visa_type')\n",
    "immigration_df.createOrReplaceTempView(\"i94_immigration_monthly\")\n",
    "immigration_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+----------------+------------------+--------------------+--------------+--------------------+--------------+----------+\n",
      "|            city|state_code|total_population|total_foreign_born|percent_foreign_born|total_veterans|    percent_veterans|household_size|median_age|\n",
      "+----------------+----------+----------------+------------------+--------------------+--------------+--------------------+--------------+----------+\n",
      "|   Silver Spring|        MD|           82463|             30908|  0.3748105210821823|          1562|0.018941828456398628|           2.6|      33.8|\n",
      "|          Quincy|        MA|           93629|             32935|  0.3517606724412308|          4147| 0.04429183265868481|          2.39|      41.0|\n",
      "|          Hoover|        AL|           84839|              8229| 0.09699548556677944|          4819|0.056801706762220204|          2.58|      38.5|\n",
      "|Rancho Cucamonga|        CA|          175232|             33878|  0.1933322680788897|          5821| 0.03321881848064281|          3.18|      34.5|\n",
      "|          Newark|        NJ|          281913|             86253| 0.30595609283715186|          5829|  0.0206765917144651|          2.73|      34.6|\n",
      "+----------------+----------+----------------+------------------+--------------------+--------------+--------------------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_demographics_df = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            City as city, `State Code` as state_code,\n",
    "            `Total Population` as total_population, `Foreign-born` as total_foreign_born,\n",
    "            (`Foreign-born` / `Total Population`) AS percent_foreign_born,\n",
    "            `Number of Veterans` as total_veterans,\n",
    "            (`Number of Veterans` / `Total Population`) AS percent_veterans,\n",
    "            `Average Household Size` AS household_size, `Median Age` AS median_age\n",
    "        FROM city_demographics_source\n",
    "\"\"\").toDF('city', 'state_code', 'total_population', 'total_foreign_born', 'percent_foreign_born',\n",
    "          'total_veterans', 'percent_veterans', 'household_size', 'median_age')\n",
    "city_demographics_df.createOrReplaceTempView(\"city_demographics\")\n",
    "city_demographics_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Perform data clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[state_code: string, state_name: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop nulls\n",
    "city_demographics_df.na.drop()\n",
    "immigration_df.na.drop()\n",
    "states_df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Create fact table, generated as summary analytics from dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------+--------------------+------------------+-----------------+------------------------------+------------------------------------+-----------------------------------+\n",
      "|state_code|avg_percent_foreign_born|avg_percent_veterans|avg_household_size|   avg_median_age|recent_period_total_immigrants|recent_period_total_immigrants_month|recent_period_total_immigrants_year|\n",
      "+----------+------------------------+--------------------+------------------+-----------------+------------------------------+------------------------------------+-----------------------------------+\n",
      "|        AK|     0.11134434791342337| 0.09204037563400794|              2.77|             32.2|                          1604|                              2016.0|                                4.0|\n",
      "|        AL|     0.04998828196121399| 0.06790675684503047|2.4300000000000006|36.16176470588235|                          8188|                              2016.0|                                4.0|\n",
      "|        AR|     0.10948156422255319| 0.05172062842819985| 2.526896551724138|32.73793103448276|                          2873|                              2016.0|                                4.0|\n",
      "|        AZ|      0.1264198306585715| 0.06605226419604705|2.7743749999999987|35.03750000000001|                         20218|                              2016.0|                                4.0|\n",
      "|        CA|      0.2757416715942214| 0.04134256546024132|3.0953254437869817|36.17396449704136|                        470386|                              2016.0|                                4.0|\n",
      "+----------+------------------------+--------------------+------------------+-----------------+------------------------------+------------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_analysis_df = spark.sql(\"\"\"\n",
    "               \n",
    "WITH\n",
    "    i94_summary AS (\n",
    "        SELECT\n",
    "            state_code, COUNT(*) as recent_period_total_immigrants, \n",
    "            AVG(month) as recent_period_total_immigrants_month,\n",
    "            AVG(year) as recent_period_total_immigrants_year\n",
    "        FROM i94_immigration_monthly \n",
    "        GROUP BY state_code\n",
    "    ),\n",
    "    demographics_summary AS (\n",
    "        SELECT \n",
    "            state_code,\n",
    "            AVG(percent_foreign_born) as avg_percent_foreign_born,\n",
    "            AVG(percent_veterans) as avg_percent_veterans,\n",
    "            AVG(household_size) as avg_household_size,\n",
    "            AVG(median_age) as avg_median_age\n",
    "        FROM city_demographics \n",
    "        GROUP BY state_code\n",
    "    )\n",
    "    SELECT\n",
    "        demographics_summary.*, recent_period_total_immigrants,\n",
    "        recent_period_total_immigrants_year, recent_period_total_immigrants_month\n",
    "    FROM demographics_summary\n",
    "    INNER JOIN i94_summary\n",
    "    ON demographics_summary.state_code = i94_summary.state_code\n",
    "    ORDER BY demographics_summary.state_code\n",
    "    \n",
    "\n",
    "\"\"\").toDF('state_code', 'avg_percent_foreign_born', 'avg_percent_veterans', 'avg_household_size',\n",
    "          'avg_median_age', 'recent_period_total_immigrants',\n",
    "          'recent_period_total_immigrants_month', 'recent_period_total_immigrants_year'\n",
    ")\n",
    "state_analysis_df.createOrReplaceTempView(\"state_analysis\")\n",
    "state_analysis_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.4 Save final parquet files from each table created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write fact table to parquet\n",
    "states_df.write.mode('overwrite').parquet(\"states\")\n",
    "immigration_df.write.mode('overwrite').parquet(\"immigration\")\n",
    "city_demographics_df.write.mode('overwrite').parquet(\"demographics\")\n",
    "state_analysis_df.write.mode('overwrite').parquet(\"state_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.5 Run quality control checks on the final parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compare_counts_df_parquet(input_df, parquetName):\n",
    "    '''\n",
    "    Looks for missing data in input dataframe, and returns a dataframe with the number of missing values in each column'\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        input dataFrame\n",
    "    '''\n",
    "    areCountsEqual = input_df.count() == spark.read.parquet(parquetName).count()\n",
    "    print(\"Counts for \" + parquetName + (\" are\" if areCountsEqual else \" are not\") + \" the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for states are the same\n",
      "Counts for immigration are the same\n",
      "Counts for demographics are the same\n",
      "Counts for state_analysis are the same\n"
     ]
    }
   ],
   "source": [
    "compare_counts_df_parquet(states_df, \"states\")\n",
    "compare_counts_df_parquet(immigration_df, \"immigration\")\n",
    "compare_counts_df_parquet(city_demographics_df, \"demographics\")\n",
    "compare_counts_df_parquet(state_analysis_df, \"state_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compare_columns_df_parquet(input_df, parquetName):\n",
    "    '''\n",
    "    Looks for missing data in input dataframe, and returns a dataframe with the number of missing values in each column'\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        input dataFrame\n",
    "    '''\n",
    "    parquet_fields = spark.read.parquet(parquetName).schema.fields\n",
    "    df_fields = input_df.schema.fields\n",
    "\n",
    "    areCountsEqual = input_df.count() == spark.read.parquet(parquetName).count()\n",
    "    print(\"The schema fields for \" + parquetName + (\" are\" if parquet_fields == df_fields else \" are not\") + \" the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema fields for states are the same\n",
      "The schema fields for immigration are the same\n",
      "The schema fields for demographics are the same\n",
      "The schema fields for state_analysis are the same\n"
     ]
    }
   ],
   "source": [
    "#Fix nullable column mismatch, which appears later in parquet\n",
    "state_analysis_df.schema['recent_period_total_immigrants'].nullable = True\n",
    "\n",
    "# Validate that schema is the same in parquet files, compared to data frame sources\n",
    "compare_columns_df_parquet(states_df, \"states\")\n",
    "compare_columns_df_parquet(immigration_df, \"immigration\")\n",
    "compare_columns_df_parquet(city_demographics_df, \"demographics\")\n",
    "compare_columns_df_parquet(state_analysis_df, \"state_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.6 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from.\n",
    "\n",
    "\n",
    "##### states\n",
    "\n",
    "| Column    | Type    | Description                                     |\n",
    "|-----------|---------|-------------------------------------------------|\n",
    "| state_code | date    | Primary key. Full date in the format YYYY-MM-DD |\n",
    "| title       | integer | Day of the month of full_date                   |\n",
    "\n",
    "\n",
    "##### i94_immigration_monthly\n",
    "\n",
    "| Column   | Type    | Description                             |\n",
    "|----------|---------|-----------------------------------------|\n",
    "| i94_id    | integer    | Id                                  |\n",
    "| year    | integer | Year of arrival                          |\n",
    "| month   | integer | Month of arrival                         |\n",
    "| city_port_code   | integer | Entrant origin country code     |\n",
    "| state_code  | string  | US state code of arrival             |\n",
    "| birth_year  | date    | Birth year of applicant              |\n",
    "| gender   | string  | Gender / sex of entrant                 |\n",
    "| visatype | string  | Type of visa granted                    |\n",
    "\n",
    "##### city_demographics\n",
    "\n",
    "| Column            | Type    | Description                                                |\n",
    "|-------------------|---------|------------------------------------------------------------|\n",
    "| city              | string  | US city name                                               |\n",
    "| state_code        | string  | State where the city is located                            |\n",
    "| total_population  | integer | Total population in city                                   |\n",
    "| total_foreign_born| integer | Number of people born outside the US                       |\n",
    "| percent_foreign_born  | double | Percent of people born outside the US                  |\n",
    "| total_veterans    | integer | Total veteran population in city                           |\n",
    "| percent_veterans    | double | Percent veteran population in city                       |\n",
    "| household_size    | integer | Average size of the household                              |\n",
    "\n",
    "##### state_analysis\n",
    "\n",
    "| Column   | Type    | Description                             |\n",
    "|----------|---------|-----------------------------------------|\n",
    "| state_code                           | integer | State abbreviation                         |\n",
    "| avg_percent_foreign_born             | float   | Average of cities foreign born percent     |\n",
    "| avg_percent_veterans                 | float   | Average of cities veterans percent         |\n",
    "| avg_household_size                   | float   | Average of cities household size           |\n",
    "| avg_median_age                       | integer | Average of cities median age               |\n",
    "| recent_period_total_immigrants       | integer | Total number of immigrants in recent period|\n",
    "| recent_period_total_immigrants_month | integer | Recent period month from i94 data          |\n",
    "| recent_period_total_immigrants_year  | integer | Recent period year from i94 data           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5.1 Rationale for choice of tools and technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Python has a number of libraries available for data engineering, and a robust set of built in tools for data processing.\n",
    "- Pandas facilitates powerful manipulation of data frames.\n",
    "- Spark is a specialized tool for manipulating large data sets from a wide array of possible different sources. This enables efficient data wrangling and scaling potential with the use of clusters of nodes.\n",
    "\n",
    "In the future, we may also incorporate Apache Airflow for better management and monitoring of DAG pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5.2 How often data should be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "That data used in this sample is now quite dated, and is intended for demonstration purposes only. For actual production scenarios, new data sets should be downloaded and provided for the pipelines on a periodic basis. The demographics data is generated from census data, which is provided every 10 years; therefore this data should be refreshed every 10 years. The immigration data contains information for one month; therefore, this data should be refreshed monthly, as new monthly data is continuously published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5.3 Scenario: Data increased by 100x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Because we are using Spark, we have the option of deploying additional nodes so that we can handle more compute intensive requirements, such as source data being increased by 100 fold. Additionally, we may use EC2 clusters for additional compute power if our current infrastructure does not support the increased load. However, the use of cloud technologies such as EC2 will also imply additional operational expenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5.4 Scenario: Data populates a dashboard that must be updated on a daily basis by 7am every day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In order for us to populate a dashboard that must be updated on a daily basis at 7 am every day, we have two main options. The first option is to  use a task scheduler like crontab to automatically run this Python script at the desired schedule. Another option is to make use of Apache Airflow. Aside from the other benefits of Apache Airflow previously indicated, it also has native support for running pipelines on a preset schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5.5 Scenario: Database needs to be accessed by 100+ people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Because we have made use of the Star Schema, this data is already optimized for use by a large number of people, such as more than 100 people. Note that we would need to define user roles for this audience, such as Read only access. In case we would like to further optimize accessibility, we may also consider hosting this data in the cloud. This option could be particularly useful if the business does not already have the infrastructure in place for hosting databases for access by a wide number of people. However, again please note that the use of cloud technologies will also imply increased operational expenses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
